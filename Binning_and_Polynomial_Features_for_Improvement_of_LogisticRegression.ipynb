{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UJQ-sTF7NhRU"
      ],
      "authorship_tag": "ABX9TyMQqXgbESaR6zGOaRI1JYPv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adichat08/Support-Vector-Classifier-for-Predicting-Survival-Likelihood-of-Hepatitis-Patients/blob/main/Binning_and_Polynomial_Features_for_Improvement_of_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJQ-sTF7NhRU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Binning and Polynomial Features for Improvement of LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btqao3U5X7x_"
      },
      "source": [
        "Linear models, as implied by the name, usually model a linear relationship between features and the target. One thing that can make this linear relationship more complex is binning of certain features in the dataset. Essentially, binning will split each feature up into multiple new features, allowing the model to make a representation for each one of those new features. In this case, it is likely to add some complexity to the model, allowing it to consider many more factors about the input data when making decisions.\n",
        "\n",
        "Adding polynomial features, or powers of the original feature, can also, in many cases, help improve performance by changing the linear model's representation of a feature from a line to a curve. This will create a more complex model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ppAN2D3q4q7",
        "outputId": "15ac855e-e82b-46b0-c30e-ecb43ddf6115"
      },
      "source": [
        "# applying the column transformer to scale the data\n",
        "X_binned = ct.fit_transform(X_train)\n",
        "# creating the KBinsDiscretizer object to split each features into 5 new ones\n",
        "kb = KBinsDiscretizer(n_bins=5,strategy='quantile')\n",
        "# applying the KBinsDiscretizer object on the training data\n",
        "kb.fit(X_binned)\n",
        "X_binned = kb.transform(X_binned)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
            "  'decreasing the number of bins.' % jj)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTXlIXOurjOp",
        "outputId": "03096fa1-8c8e-4ec3-a5bb-3b42d9054cae"
      },
      "source": [
        "# Running a cross validation with the linear model on the binned data\n",
        "kfold = KFold(n_splits=3,shuffle=True,random_state=42)\n",
        "log = LogisticRegression(random_state=42)\n",
        "print(\"Cross-validation scores:\\n{}\".format(\n",
        "      cross_val_score(log,X_binned,y_train,cv=kfold)))\n",
        "print('Average score:\\n{}'.format(\n",
        "    cross_val_score(log,X_binned,y_train,cv=kfold).mean()))\n",
        "print(\"Cross-validation AUC:\\n{}\".format(\n",
        "      cross_val_score(log,X_binned,y_train,cv=kfold,scoring='roc_auc')))\n",
        "print('Average AUC:\\n{}'.format(\n",
        "    cross_val_score(log,X_binned,y_train,cv=kfold,scoring='roc_auc').mean()))\n",
        "print('F1 average:\\n{}'.format(\n",
        "    cross_val_score(log,X_binned,y_train,cv=kfold,scoring = 'f1').mean()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation scores:\n",
            "[0.82051282 0.92307692 0.79487179]\n",
            "Average score:\n",
            "0.8461538461538461\n",
            "Cross-validation AUC:\n",
            "[0.875      0.86290323 0.78571429]\n",
            "Average AUC:\n",
            "0.841205837173579\n",
            "F1 average:\n",
            "0.5204795204795205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z67ZFVQlQ3a_"
      },
      "source": [
        "# adding polynomial features(degree 5) of the original dataset\n",
        "poly = PolynomialFeatures(degree=5, include_bias=False)\n",
        "poly.fit(X_binned)\n",
        "X_poly = poly.transform(X_binned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP_vtBH4PEDb",
        "outputId": "138fd15b-5ffb-4887-ac9f-cc3115733c4e"
      },
      "source": [
        "# Running a cross validation with the linear model on the data holding the polynomial features\n",
        "kfold = KFold(n_splits=3,shuffle=True,random_state=42)\n",
        "log = LogisticRegression(random_state=42,max_iter=1000,)\n",
        "print(\"Cross-validation scores:\\n{}\".format(\n",
        "      cross_val_score(log,X_poly,y_train,cv=kfold)))\n",
        "print('Average score:\\n{}'.format(\n",
        "    cross_val_score(log,X_poly,y_train,cv=kfold).mean()))\n",
        "print(\"Cross-validation AUC:\\n{}\".format(\n",
        "      cross_val_score(log,X_poly,y_train,cv=kfold,scoring='roc_auc')))\n",
        "print('Average AUC:\\n{}'.format(\n",
        "    cross_val_score(log,X_poly,y_train,cv=kfold,scoring='roc_auc').mean()))\n",
        "print('F1 average:\\n{}'.format(\n",
        "    cross_val_score(log,X_poly,y_train,cv=kfold,scoring = 'f1').mean()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation scores:\n",
            "[0.84615385 0.82051282 0.84615385]\n",
            "Average score:\n",
            "0.8376068376068376\n",
            "Cross-validation AUC:\n",
            "[0.79017857 0.84274194 0.81696429]\n",
            "Average AUC:\n",
            "0.8166282642089094\n",
            "F1 average:\n",
            "0.4444444444444445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox9g3heVFSV_"
      },
      "source": [
        "Neither binning nor adding polynomials of the original features appear to improve the model's performance on any of the key metrics. This could be because LogisticRegression doesn't simply draw a line for its feature representations, and is likely to build more complex models than, say, LinearRegression. It's also a possibility that there is a fairly linear relationships between the features and the target."
      ]
    }
  ]
}